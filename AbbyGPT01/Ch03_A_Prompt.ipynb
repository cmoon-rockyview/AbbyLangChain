{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "#API Key Load\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('AbbyUtils.py')\n",
    "from AbbyUtils import langsmith\n",
    "langsmith('AbbyCh03', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model = \"gpt-4o-mini\", temperature=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1. Create a PromptTemplate object using the from_template() method\n",
    "\n",
    "- Define the template by enclosing the variables to be substituted in `{ variable }`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"Tell me about {topic} in 100 words or less.\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "prompt = prompt.format(topic=\"the history of the Roman Empire\")\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reponse = llm.invoke(prompt)\n",
    "reponse.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "template = \"Tell me about {topic} in 100 words or less.\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chain.invoke(\"city of abbotsford\")\n",
    "response = chain.invoke({\"topic\": \"city of abbotsford\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yaml vs Json\n",
    "https://www.notion.so/YAML-vs-JSON-Key-Differences-1982aa64aeb580708566fc9e0205f758\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Template from a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import load_prompt\n",
    "\n",
    "prompt_capital = load_prompt(\"prompts/country_capital.yaml\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_capital = prompt_capital | llm\n",
    "reponse_capital = chain_capital.invoke({\"country\": \"Mauritius\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reponse_capital.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_abbyEmail = load_prompt(\"prompts/AbbyCC.yaml\", encoding=\"utf-8\")\n",
    "prompt_abbyEmail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_content = \"\"\"\n",
    "\n",
    "From: John Surjadi <surjadigroup@gmail.com> \n",
    "Sent: Friday, July 5, 2024 6:31 PM\n",
    "To: Ross Siemens <RSiemens@abbotsford.ca>\n",
    "Subject: RE: gasoline gouging\n",
    "\n",
    "I would like the gouging in Abbotsford to stop, today the prices were cheaper in Vancouver than they were in Abbotsford and as the mayor of Abbotsford I would think that you would have something to say about this... I'm not happy to know that our Mayor is not concerned enough to regulate how business is being conducted in Abbotsford BC... We are already taxed to death and this is completely out of control when gasoline prices in Abbotsford are higher than Metro Vancouver...\n",
    "I have reached out to you in the past, I have sent you emails regarding this problem and just like every other politician, you have pass the buck and told me that you do not control the gas pricing in Abbotsford... Notes you are also in the gasoline business... well it's about time our Mayor took interest and the concerns of your voters and started to do something to contain this matter... \n",
    "\n",
    "I will no longer vote for you in the future if something is not done about this immediately this year... \n",
    "It's bad enough we have a carbon tax...\n",
    "\n",
    "Yours truly Richard Williams\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "outputParser = StrOutputParser()\n",
    "chain_abbyEmail = prompt_abbyEmail | llm | outputParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain_abbyEmail.stream({\"email_content\": email_content})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm_gemini = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro-latest\")\n",
    "\n",
    "chain_abbyEmail_gemini = prompt_abbyEmail | llm_gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain_abbyEmail_gemini.invoke({\"email_content\": email_content})\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
